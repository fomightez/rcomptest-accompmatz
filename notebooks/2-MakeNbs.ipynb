{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating RCompanion notebooks from extracted markdown via notedown\n",
    "\n",
    "Follows from `one`.\n",
    "\n",
    "Meant to be run in sessions launched from [here](https://github.com/fomightez/rcomp_testenv). \n",
    "\n",
    "## Preparation\n",
    "\n",
    "### STEP #1: Get this notebook running in a freshly-launched Binder session \n",
    "\n",
    "**WHERE TO RUN:**  \n",
    "Run this in a Binder session launched from anywhere since this doesn't include any fancy dependencies not handled by pip.\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT NEEDED HERE. FROM SOURCE NOTEBOOK. KEEPING FOR NOW BECAUSE MAY EVENTUALLY SOMETHING SIMILAR(?):\n",
    "#\n",
    "'''\n",
    "# For Clearing out the contents presently there so things aren't as cluttered. \n",
    "# But don't delete this notebook.\n",
    "from shlex import quote\n",
    "pathname_of_file_to_keep = quote(\"notebooks/Generating later Circos tutorial notebooks from extracted markdown via notedown and papermill.ipynb\")\n",
    "name_of_file_to_keep = quote(\"Generating later Circos tutorial notebooks from extracted markdown via notedown and papermill.ipynb\")\n",
    "\n",
    "# based on Olivier Dulac's comment at https://unix.stackexchange.com/questions/153862/remove-all-files-directories-except-for-one-file\n",
    "%cd ..\n",
    "!cp $pathname_of_file_to_keep .\n",
    "!rm -rf notebooks\n",
    "!mkdir notebooks\n",
    "!mv $name_of_file_to_keep notebooks/\n",
    "%cd notebooks\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP #2: Unpack previous results\n",
    "\n",
    "**REQUIRED FILES**\n",
    "\n",
    "Besides this notebook (and installed notedown packages) the following must be uploaded to the running binder session:\n",
    "\n",
    "- `FirstSetmarkdown_from_RCompanion.tar.gz` from `Getting R Companion html and converting to markdown text.ipynb` . This contains markdowns made for each of the pages, plus two pertinent lists stored as json.\n",
    "\n",
    "Unpack that. And bring in the list of urls and files into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "unpacked_example = os.path.join(\".\",\"RCompanion_urls_to_get.json\")\n",
    "file_needed = \"FirstSetmarkdown_from_RCompanion.tar.gz\"\n",
    "\n",
    "import sys\n",
    "if os.path.isfile(unpacked_example):\n",
    "    sys.stderr.write(\"\\nAppears '{}' has already been unpacked.\\n\".format(file_needed))\n",
    "elif os.path.isfile(file_needed):\n",
    "    !tar xzf {file_needed}\n",
    "else:\n",
    "    sys.stderr.write(\"\\n\\n*****************ERROR**************************\\n\"\n",
    "        \"The file '{0}' is needed.\\n\"\n",
    "        \"Upload '{0}' to this Jupyter session and re-run this cell.\\n\"\n",
    "        \"*****************ERROR**************************\\n\".format(file_needed))\n",
    "    sys.exit(1)\n",
    "stored_json_files_and_names = [\n",
    "    \"RCompanion_markdowns_made.json\",\n",
    "    \"RCompanion_urls_to_get.json\"\n",
    "                            ]\n",
    "stored_json_files_and_names_to_unpack_as = {k:f'{k.split(\"_\")[0]}_{k.split(\"_\")[1]}' for k in stored_json_files_and_names}\n",
    "import json\n",
    "for k,v in stored_json_files_and_names_to_unpack_as.items():\n",
    "    with open(k, 'r') as f:\n",
    "        globals()[v] = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REQUIRED MODULES/PACKAGES**\n",
    "Install notedown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install papermill\n",
    "!pip install notedown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP #3: Step through the markdown files and fix some issues:\n",
    "\n",
    "List of fixes:\n",
    "\n",
    "- fix `b_10.md` to look like the other pages in regard to lines that begin `title: '`.<--- I think altering how handling the spaces has made this moot now. But it looks like I may need to address the doubling of the dashes if that matters.\n",
    "- remove most everything except what is marked as `::: {#content}`, marked by ending at `::: {#footer}`. The only other stuff to keep is what is right near the top on the line that begins `title: '` (maked by having `'` after it), the  line after `{#mastword}` (marked by having `Salvatore S. Mangiafico` after it), and the two lines after `::: {#title}`(marked by having `:::` after it)..<--- I think altering how handling the spaces has made this need to be altered, too.\n",
    "- I'd like the headings which currently have string starting `_toc` on the lines be level theee headings, i.e., have `###`.\n",
    "- remove any lines that are just \":::\" if strip away empty space.\n",
    "- remove lines reading 'Advertisement' or starting with 'medianet'.\n",
    "- remove occurence double-dashes as long as not a long string of dashes or \"pp.\" on the line.\n",
    "- replace non-breaking spaces, and spaces in results lines, that where filled with placeholder and not spaces so wouldn't introduce line breaks when pandoc converted (seems pandoc doesn't like numbers in line with spaces and makes a new line?)\n",
    "- finalize results lines\n",
    "- put code fences where they belong\n",
    "- convert `###\\n--------------------------------------------------------------` to \n",
    "`###--------------------------------------------------------------`. In other words remove the line break after the three asterisks. <--- actually now handled earlier (and differenly) and more generally by `avoid_line_breaks_caused_by_comments()`\n",
    "- lines beginning `ASxSPACE###` get changed to beginning with `###`. Mainly pertinent to lines like, ``### --------------------------------------------------------------`\n",
    "\n",
    "(**Note for trouble-shooting this section and all after:  \n",
    "Each time I want to redo the next cell and ones after, the easiest way to get back to this point was to first make a separate directory below the one with this notebook called `unpacked_original` where I uploaded `FirstSetmarkdown_from_RCompanion.tar.gz` and then unpacked it with `tar -xzf FirstSetmarkdown_from_RCompanion.tar.gz`. Then to reset, I just ran in a terminal `cp *.md ../.` where the terminal working directory was presently `unpacked_original`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"??????????\"\n",
    "\n",
    "\n",
    "def fix_first_title_text(md):\n",
    "    '''\n",
    "    fix extracted markdown to look like the other pages in regard to lines that begin `title: '`\n",
    "    '''\n",
    "    with open(md, 'r') as input:\n",
    "        all_md=input.read()\n",
    "    new_title_line = all_md.split(\"title: |\")[1].split(\"---\")[0].strip().replace(\"\\n\",\" \").replace(\"     \",\" \")\n",
    "    first_pt = all_md.split(\"title: |\")[0]\n",
    "    the_last_pt = all_md.split(\"title: |\")[1].split(\"---\",1)[1]\n",
    "    new_md = f\"{first_pt}title: '{new_title_line}'\\n{the_last_pt}\" \n",
    "    return new_md\n",
    "\n",
    "def reduce_to_content(md):\n",
    "    '''\n",
    "    Reduces to just content section except for keeping titles.\n",
    "    \n",
    "    Adds a footer placeholder for use later.\n",
    "    '''\n",
    "    with open(md, 'r') as input:\n",
    "        all_md=input.read()\n",
    "    #get three things I need and then append to contents section\n",
    "    first_pt = all_md.split(\"title: '\")[1].split(\"'\")[0].strip()\n",
    "    first_pt += \"\\n=============================\\n\\n\"\n",
    "    '''\n",
    "    second_pt = all_md.split(\"{#mastword}\")[1].split(\"Salvatore S. Mangiafico\")[0].strip()\n",
    "    second_pt+= \"\\n\\nby Salvatore S. Mangiafico\\n\\n\"\n",
    "    '''\n",
    "    second_pt = (\"An R Companion for the Handbook of Biological Statistics\\n--\"\n",
    "                 \"-------------------------------------------------------------------\")\n",
    "    second_pt+= \"\\n\\nby Salvatore S. Mangiafico\\n\\n\"\n",
    "    '''\n",
    "    third_pt = all_md.split(\"::: {#title}\")[1].split(\":::\")[0].strip()\n",
    "    third_pt += \"\\n\\n\"\n",
    "    \n",
    "    content = all_md.split(\"::: {#content}\")[1].split(\"::: {#footer}\")[0].strip()\n",
    "    '''\n",
    "    content = all_md.split(\n",
    "        \"Summary and Analysis of Extension Program Evaluation in R\")[1].split(\n",
    "        \"Â©2015 by Salvatore S. Mangiafico\")[0].strip()\n",
    "    footer_placeholder = \"\\n RCOMPANION_FOOTER_GOES_HERE\"\n",
    "    #new_md = f\"{first_pt}{second_pt}{third_pt}{content}{footer_placeholder}\" \n",
    "    new_md = f\"{first_pt}{second_pt}{content}\\n{footer_placeholder}\" \n",
    "    return new_md\n",
    "\n",
    "\n",
    "def reformat_toc_headings(md):\n",
    "    '''\n",
    "    Takes a markdown text file and converts the _toc headings \n",
    "    to leavel three markdown hardings\n",
    "    so that they will be rendered as headings that jupyter-book can use.\n",
    "    \n",
    "    \n",
    "    Examples:\n",
    "    Inputs:\n",
    "    ### []{#_Toc420844544}[Post-hoc tests]{#_Toc411541425}\n",
    "    [Null hypothesis]{#_Toc411541423}\n",
    "    #### [Post-hoc pairwise G-tests with]{#_Toc420844545} [RVAideMemoire]{style=\"font-style:normal\"}\n",
    "    \n",
    "    Outputs:\n",
    "    ### Post-hoc tests ###\n",
    "    ### Null hypothesis ###\n",
    "    ### Post-hoc pairwise G-tests with RVAideMemoire ###\n",
    "    \n",
    "    Seems regex `\\[.*?\\]` is what I need according to https://www.regextester.com/97589\n",
    "    \n",
    "    Returns the updated text for replacing the original markdown file. The reason\n",
    "    I don't replace in the function is because it seems `%store` is line magic and\n",
    "    cannot access the variables defined locally within a function even if\n",
    "    invoked in a function. Although I could do it if I made it a global variable like I \n",
    "    somtimes d, but doing it this way since based on older code.\n",
    "    '''\n",
    "    new_md = \"\"\n",
    "    with open(md, 'r') as input:\n",
    "        for line in input:\n",
    "            # While have lines going to remove front space from some of\n",
    "            # the lines that beging with multiple pound symbols\n",
    "            if line.startswith(r'ASxSPACE###'):\n",
    "                line = line[len('ASxSPACE'):]\n",
    "            if ((\"#_Toc\") in line):\n",
    "                bracketed_text = ' '.join(re.findall('\\[(.*?)\\]', line))\n",
    "                new_md += f\"### {bracketed_text} ###\"\n",
    "            else:\n",
    "                new_md += line\n",
    "    return new_md\n",
    "\n",
    "def remove_triple_colon_lines(md):\n",
    "    '''\n",
    "    Remove any lines that are just \":::\" if strip away empty space.\n",
    "    \n",
    "    While at that, also remove any lines reading \n",
    "    'Advertisement' or starting with 'medianet'.\n",
    "    \n",
    "    Also in this one handle double-dashes as long as not a long string of dashes or \"pp.\" on the line.\n",
    "    '''\n",
    "    new_md = \"\"\n",
    "    with open(md, 'r') as input:\n",
    "        for line in input:\n",
    "            if \"--\" in line and \"----------\" not in line and \"pp.\" not in line:\n",
    "                line = line.replace('--','-')\n",
    "            if line.strip() == \":::\" or (\n",
    "                line.strip() == \"Advertisement\") or (\n",
    "                line.strip().startswith(\"medianet\")):\n",
    "                continue\n",
    "            else:\n",
    "                new_md += line\n",
    "\n",
    "    return new_md\n",
    "\n",
    "def put_footer_in_placefolder(md,footer_text):\n",
    "    '''\n",
    "    Puts footer text into placeholder from earlier.\n",
    "    '''\n",
    "    footer_placeholder = r\"\\n \\n RCOMPANION_FOOTER_CODE_GOES_HERE\"\n",
    "    with open(md, 'r') as input:\n",
    "        all_md=input.read()\n",
    "    new_md = all_md.replace(footer_placeholder,footer_text)\n",
    "    return new_md\n",
    "\n",
    "def put_in_code_fences(md):\n",
    "    '''\n",
    "    put in code fences\n",
    "    '''\n",
    "    start_placeholder_tag = u\"RCOMPxCODExFENCExSTARTSxHERE\"\n",
    "    end_placeholder_tag = u\"RCOMPxCODExFENCExENDSxHERE\"\n",
    "    R_code_fence_start = \"```{.python .input}\"\n",
    "    code_fence_end = \"```\"\n",
    "    with open(md, 'r') as input:\n",
    "        all_md=input.read()\n",
    "    new_md = all_md.replace(start_placeholder_tag,R_code_fence_start)\n",
    "    new_md = new_md.replace(end_placeholder_tag,code_fence_end)\n",
    "    new_md = new_md.replace(r'#     #     #',' ') # remove `#     #     #` occurrences because have cell borders now\n",
    "    new_md = new_md.replace(r'#ASxSPACEASxSPACEASxSPACEASxSPACE #ASxSPACEASxSPACEASxSPACEASxSPACE #',' ') # remove `#     #     #` occurrences because have cell borders now\n",
    "    new_md = new_md.replace(r'#ASxSPACEASxSPACEASxSPACEASxSPACEASxSPACE#ASxSPACEASxSPACEASxSPACEASxSPACEASxSPACE#',' ') # remove `#     #     #` occurrences because have cell borders now\n",
    "    #while changing code blocks, remove the leading or trailing blank lines\n",
    "    without_blank_lines = \"\"\n",
    "    for p in new_md.split(R_code_fence_start):\n",
    "        if code_fence_end in p:\n",
    "            code_and_not_code_parts = p.split(code_fence_end)\n",
    "            if code_and_not_code_parts[0].strip().endswith(r'\\n    \\n```'):\n",
    "                without_blank_lines += (R_code_fence_start + \"\\n\" +\n",
    "                                        code_and_not_code_parts[0].strip()[:-len(r'\\n    \\n```')] + \n",
    "                                         \"\\n\" + code_fence_end + \"\\n\" + \n",
    "                                        code_and_not_code_parts[1] )\n",
    "            elif code_and_not_code_parts[0].strip().endswith(r'   \\n\\n \\n```'):\n",
    "                without_blank_lines += (R_code_fence_start + \"\\n\" +\n",
    "                                        code_and_not_code_parts[0].strip()[:-len(r'   \\n\\n \\n```')] + \n",
    "                                         \"\\n\" + code_fence_end + \"\\n\" + \n",
    "                                        code_and_not_code_parts[1] )\n",
    "            else:\n",
    "                without_blank_lines += (R_code_fence_start + \"\\n\" +\n",
    "                                        code_and_not_code_parts[0].strip() + \n",
    "                                         \"\\n\" + code_fence_end + \"\\n\" + \n",
    "                                        code_and_not_code_parts[1] )\n",
    "        else:\n",
    "            without_blank_lines += p\n",
    "    new_md = without_blank_lines\n",
    "    new_md = new_md.replace(R_code_fence_start+\"\\\\\",\"R_code_fence_start\")\n",
    "    new_md = new_md.replace(code_fence_end+\"\\\\\",\"code_fence_end\")\n",
    "    # fix any casses where the start of the code fence is not at the start of a line because it should be\n",
    "    with_fences_at_start = ''\n",
    "    for line in new_md.split(\"\\n\"):\n",
    "        if R_code_fence_start in line and not line.strip().startswith(R_code_fence_start):\n",
    "            parts_o_line = line.strip().split(R_code_fence_start)\n",
    "            with_fences_at_start += parts_o_line[0] + \"\\n\" + R_code_fence_start + \"\\n\" + parts_o_line[1] + \" \".join(parts_o_line[2:])\n",
    "        else:\n",
    "            with_fences_at_start += line + \"\\n\"\n",
    "    new_md = with_fences_at_start\n",
    "    \n",
    "    return new_md\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "'''\n",
    "needs_fixin_title = [\"b_10.md\"]\n",
    "for md in RCompanion_markdowns:\n",
    "    if md in needs_fixin_title:\n",
    "        new_md = fix_first_title_text(md)\n",
    "        %store new_md > temp.txt \n",
    "        !mv temp.txt {md}\n",
    "        sys.stderr.write(\"First title text in {} fixed\"\n",
    "            \".\\n\".format(md))\n",
    "'''\n",
    "for md in RCompanion_markdowns:\n",
    "    print(md)\n",
    "    new_md = reduce_to_content(md)\n",
    "    %store new_md > temp.txt \n",
    "    !mv temp.txt {md}\n",
    "    sys.stderr.write(\"Text extracted from {} has been reduced to content\"\n",
    "        \".\\n\".format(md))\n",
    "for md in RCompanion_markdowns:\n",
    "    new_md = reformat_toc_headings(md)\n",
    "    %store new_md > temp.txt \n",
    "    !mv temp.txt {md}\n",
    "    sys.stderr.write(\"Any '_toc' labeled headings in {} have been reformatted\"\n",
    "        \".\\n\".format(md))\n",
    "for md in RCompanion_markdowns:\n",
    "    new_md = remove_triple_colon_lines(md)\n",
    "    %store new_md > temp.txt \n",
    "    !mv temp.txt {md}\n",
    "    sys.stderr.write(\"Any lines that are just ':::' in {} have been removed\"\n",
    "        \" and\\ndouble-dashes reduced to single\"\n",
    "        \".\\n\".format(md))\n",
    "    sys.stderr.write(\"Lines reading 'Advertisement' or starting with 'medianet'\\nin {} have been removed\"\n",
    "        \".\\n\".format(md))\n",
    "for md in RCompanion_markdowns:\n",
    "    new_md = put_in_code_fences(md)\n",
    "    %store new_md > temp.txt \n",
    "    !mv temp.txt {md}\n",
    "    sys.stderr.write(\"Code fences have been marked in {}\"\n",
    "        \".\\n\".format(md))\n",
    "start_placeholder_tag = \"RCOMPxRESULTxLINExSTART\"\n",
    "end_placeholder_tag = \"RCOMPxRESULTxLINExEND\"\n",
    "bracket_start_plchlder = \"BRACKETxSTARTxPLCHLDR\"\n",
    "bracket_end_plchlder = \"BRACKETxENDxPLCHLDR\"\n",
    "space_placeholder = \"ASxSPACE\"\n",
    "for md in RCompanion_markdowns:\n",
    "    with open(md, 'r') as input:\n",
    "        all_md=input.read()\n",
    "    new_md = all_md.replace(start_placeholder_tag,'<font color = \"#B22222\">')\n",
    "    new_md = new_md.replace(end_placeholder_tag,'</font><br>')\n",
    "    new_md = new_md.replace(bracket_start_plchlder,\"[\")\n",
    "    new_md = new_md.replace(bracket_end_plchlder,\"]\")\n",
    "    new_md = new_md.replace(space_placeholder,\" \")\n",
    "    #new_md = new_md.replace('###\\n------------------------------', '###------------------------------') # made moot by `avoid_line_breaks_caused_by_comments()` function in previous nb\n",
    "    #Run a few last replace commands to lessen the amount of whitespace among the results, especially at ends (note: to get this editing\n",
    "    # right, I had to run test on just single `.md` file from previous notebook to see CANNOT use raw attribute,`r`, for strings here. SEEMS\n",
    "    # INCONSISTENT WHEN YOU NEED OR NOT AND HAVE TO ALWAYS TEST. (I thought I used raw for other than regex?))\n",
    "    new_md = new_md.replace('\\n\\n<font color = \"#B22222\">','\\n<font color = \"#B22222\">')\n",
    "    new_md = new_md.replace('<font color = \"#B22222\"> </font><br>\\n \\n\\n \\n\\n','\\n')\n",
    "    new_md = new_md.replace('<font color = \"#B22222\"> </font><br>\\n\\n\\n\\n\\n','\\n')\n",
    "    new_md = new_md.replace('\\n<font color = \"#B22222\"> </font><br>\\n','\\n')\n",
    "    new_md = new_md.replace('</font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n\\n\"> <font><br>\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('<font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n \\n\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('</font><br>\\n<font color = \"#B22222\"></font><br>\\n<font color = \"#B22222\"></font><br>\\n \\n\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('</font><br>\\n \\n<font color = \"#B22222\"></font><br>  \\n\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('<font><br>\\n<font color = \"#B22222\"> </font><br>\\n','</font><br>\\n')\n",
    "    # repeat the series to catch any newer occurences made by the first round\n",
    "    new_md = new_md.replace('\\n\\n<font color = \"#B22222\">',u'\\n<font color = \"#B22222\">')\n",
    "    new_md = new_md.replace('<font color = \"#B22222\"> </font><br>\\n \\n\\n \\n\\n','\\n')\n",
    "    new_md = new_md.replace('<font color = \"#B22222\"> </font><br>\\n\\n\\n\\n\\n','\\n')\n",
    "    new_md = new_md.replace('\\n<font color = \"#B22222\"> </font><br>\\n','\\n')\n",
    "    new_md = new_md.replace('</font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n\\n\"> </font><br>\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('</font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n<font color = \"#B22222\"> </font><br>\\n \\n\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('</font><br>\\n<font color = \"#B22222\"></font><br>\\n<font color = \"#B22222\"></font><br>\\n \\n\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('</font><br>\\n \\n<font color = \"#B22222\"></font><br>  \\n\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('</font<br>\\n<font color = \"#B22222\"> </font><br>\\n','</font><br>\\n')\n",
    "    new_md = new_md.replace('</font><br>\\n<font color = \"#B22222\"> </font><br>\\n','</font><br>\\n')\n",
    "    # fix cases like for `fisher.test` on `b_10.html` where want `\\n` in text and not `\\\\n` or a line break\n",
    "    new_md = new_md.replace(r'\"\\\\n\"',r'\"\\n\"') # `.replace('\"\\\\\\\\n\"',r'\"\\n\"')` also works, i.e., double-escaping for find\n",
    "    # fix a few cases of typos found\n",
    "    if md == 'h_01.md':\n",
    "        new_md = new_md.replace('require(lsmeans){','require(lsmeans)){') #addresses a current typo in first code block of Salvatore's `h_01.html`\n",
    "    %store new_md > temp.txt \n",
    "    !mv temp.txt {md}\n",
    "    sys.stderr.write(\"Non-breaking spaces, and spaces in results lines, substituted with placeholders earlier and\\nleft up until now to avoid introducing errors have been replaced\\nwith spaces in {}\"\n",
    "        \".\\n\".format(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom fixes for particular markdowns that don't conform for whatever reason\n",
    "a_06orig1 ='''install.packages(\"psych\")'''\n",
    "a_06new1 ='''# NEXT LINE COMMENTED OUT BECAUSE WILL CAUSE HANG UP WHEN RUNNING AUTOMATED\n",
    "#install.packages(\"psych\")'''\n",
    "a_06orig2 ='''```{.python .input}\n",
    "D2 = read.table(\"male-female.dat\", header=TRUE)\n",
    "```\n",
    "\n",
    "would read in data from a file called *male-female.dat*\n",
    "found in the working directory.  In this case the file could be a\n",
    "space-delimited text file:\n",
    "\n",
    " \n",
    "\n",
    "```{.python .input}\n",
    "Sex      Height\n",
    "\n",
    "male     175\n",
    "\n",
    "male     176\n",
    "\n",
    "female   162\n",
    "\n",
    "female   165\n",
    "```\n",
    "  \n",
    "\n",
    "Or\n",
    "\n",
    " \n",
    "\n",
    "```{.python .input}\n",
    "D2 = read.table(\"male-female.csv\", header=TRUE,\n",
    "sep=\",\")\n",
    "```\n",
    "  \n",
    "\n",
    "for a comma-separated file.\n",
    "\n",
    " \n",
    "\n",
    "```{.python .input}\n",
    "Sex,Height\n",
    "\n",
    "male,175\n",
    "\n",
    "male,176\n",
    "\n",
    "female,162\n",
    "\n",
    "female,165\n",
    "```\n",
    "  \n",
    "\n",
    "```{.python .input}\n",
    "D2\n",
    "```\n",
    "<font color = \"#B22222\">     Sex Height</font><br>\n",
    "<font color = \"#B22222\">1   male    175</font><br>\n",
    "<font color = \"#B22222\">2   male    176</font><br>\n",
    "<font color = \"#B22222\">3 female    162</font><br>\n",
    "<font color = \"#B22222\">4 female    165</font><br>\n",
    " \n",
    "\n",
    "R Studio also has an easy interface in the *Tools* menu\n",
    "to import data from a file.\n",
    "\n",
    " \n",
    "\n",
    "The *getwd* function will show the location of the\n",
    "working directory, and *setwd* can be used to set the working directory.\n",
    "\n",
    " \n",
    "\n",
    "```{.python .input}\n",
    "getwd()\n",
    "```'''\n",
    "a_06new2 ='''```R\n",
    "# FILE MAKING ADDED SO THE NOTEBOOK WORKS FOR AUTOMATED RUN\n",
    "Input =(\"\n",
    "Sex     Height\n",
    "male    175\n",
    "male    176\n",
    "female  162\n",
    "female  165\n",
    "\")\n",
    "\n",
    "fileConn<-file(\"male-female.dat\")\n",
    "writeLines(Input, fileConn)\n",
    "close(fileConn) # write block based on https://stackoverflow.com/a/2470277/8508004\n",
    "# END FILE MAKING ADDED SO THE NOTEBOOK WORKS FOR AUTOMATED RUN\n",
    "\n",
    "D2 = read.table(\"male-female.dat\", header=TRUE)\n",
    "```\n",
    "\n",
    "would read in data from a file called *male-female.dat*\n",
    "found in the working directory.  In this case the file could be a\n",
    "space-delimited text file:\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Sex      Height\n",
    "\n",
    "male     175\n",
    "\n",
    "male     176\n",
    "\n",
    "female   162\n",
    "\n",
    "female   165\n",
    "\n",
    "  \n",
    "\n",
    "Or\n",
    "\n",
    "for a comma-separated file.\n",
    "\n",
    " \n",
    "Sex,Height\n",
    "\n",
    "male,175\n",
    "\n",
    "male,176\n",
    "\n",
    "female,162\n",
    "\n",
    "female,165\n",
    "\n",
    "\n",
    "```R\n",
    "# FILE MAKING ADDED SO THE NOTEBOOK WORKS FOR AUTOMATED RUN\n",
    "Input =(\"\n",
    "Sex,Height\n",
    "\n",
    "male,175\n",
    "\n",
    "male,176\n",
    "\n",
    "female,162\n",
    "\n",
    "female,165\n",
    "\")\n",
    "\n",
    "fileConn<-file(\"male-female.csv\")\n",
    "writeLines(Input, fileConn)\n",
    "close(fileConn) # write block based on https://stackoverflow.com/a/2470277/8508004\n",
    "# END FILE MAKING ADDED SO THE NOTEBOOK WORKS FOR AUTOMATED RUN\n",
    "\n",
    "\n",
    "D2 = read.table(\"male-female.csv\", header=TRUE,\n",
    "sep=\",\")\n",
    "\n",
    "D2\n",
    "```\n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "<font color = \"#B22222\">     Sex Height</font><br>\n",
    "<font color = \"#B22222\">1   male    175</font><br>\n",
    "<font color = \"#B22222\">2   male    176</font><br>\n",
    "<font color = \"#B22222\">3 female    162</font><br>\n",
    "<font color = \"#B22222\">4 female    165</font><br>\n",
    " \n",
    "\n",
    "R Studio also has an easy interface in the *Tools* menu\n",
    "to import data from a file.\n",
    "\n",
    " \n",
    "\n",
    "The *getwd* function will show the location of the\n",
    "working directory, and *setwd* can be used to set the working directory.\n",
    "\n",
    " \n",
    "\n",
    "```R\n",
    "getwd()\n",
    "```'''\n",
    "a_06orig3 ='''```{.python .input}\n",
    "setwd(\"C:/Users/Salvatore/Desktop\")\n",
    "```'''\n",
    "a_06new3 ='''```R\n",
    "# NEXT LINE COMMENTED OUT BECAUSE NEEDS CUSTOMIZING & WILL CAUSE HANG UP WHEN RUNNING AUTOMATED\n",
    "#setwd(\"C:/Users/Salvatore/Desktop\")\n",
    "```'''\n",
    "a_07orig1 ='''```{.python .input}\n",
    "str(D1$ Sex)\n",
    "```'''\n",
    "a_07new1 ='''```R\n",
    "# DATAFRAME MAKING FROM PREVIOUS PAGE ADDED SO THE NOTEBOOK WORKS FOR AUTOMATED RUN\n",
    "Input =(\"\n",
    "Sex     Height\n",
    "male    175\n",
    "male    176\n",
    "female  162\n",
    "female  165\n",
    "\")\n",
    "\n",
    "D1 = read.table(textConnection(Input),header=TRUE)\n",
    "# END ADDED DATAFRAME MAKING FOR AUTOMATED RUN\n",
    "\n",
    "str(D1$ Sex)\n",
    "```'''\n",
    "a_08orig1 ='''RSiteSearch(\"chi-square\")'''\n",
    "a_08new1 ='''# NEXT LINE COMMENTED OUT BECAUSE WON'T WORK FROM BINDER & WILL CAUSE HANG UP WHEN RUNNING AUTOMATED\n",
    "#RSiteSearch(\"chi-square\")'''\n",
    "\n",
    "d_05orig1 ='''### options(contrasts = c(\"contr.sum\",\n",
    "\"contr.poly\"))'''\n",
    "d_05new1 ='''### options(contrasts = c(\"contr.sum\",\"contr.poly\"))'''\n",
    "d_05orig2 ='''library(lsmeans) library(multcompView)'''\n",
    "d_05new2 ='''library(lsmeans)\n",
    "library(multcompView)'''\n",
    "d_05orig3 ='''pairwise ~ Location,\n",
    "                      adjust = \"tukey\")'''\n",
    "d_05new3 ='''pairwise ~ Location,\n",
    "                      adjust = \"tukey\")\n",
    "summary(leastsquare)'''\n",
    "d_05orig4 ='''cld(leastsquare,\n",
    "    alpha   = 0.05,\n",
    "    Letters = letters,\n",
    "    adjust=\"tukey\")'''\n",
    "d_05new4 ='''library(emmeans)\n",
    "library(multcomp)\n",
    "\n",
    "em = emmeans(model,  ~ Location)\n",
    "\n",
    "cld(em,\n",
    "    alpha=0.05,\n",
    "    Letters=letters,      ### Use lower-case letters for .group\n",
    "    adjust=\"tukey\")       ### Tukey-adjusted comparisons'''\n",
    "\n",
    "\n",
    "d_06orig1 ='''```{.python .input}\n",
    "PT = PT$res\n",
    "\n",
    "PT\n",
    "\n",
    "library(rcompanion)\n",
    "\n",
    "cldList(comparison = PT$Comparison,\n",
    "        p.value    =\n",
    "PT$P.adj,\n",
    "        threshold  = 0.05)\n",
    "```'''\n",
    "d_06new1 ='''#CODE BLOCK COMMENTED OFF BECAUSE CANNOT AUTOMATE RUN DUE TO `Error: No significant differences.` AS OUTPUT.\n",
    "PT = PT$res\n",
    "\n",
    "PT\n",
    "\n",
    "library(rcompanion)\n",
    "\n",
    "cldList(comparison = PT$Comparison,\n",
    "        p.value    =\n",
    "PT$P.adj,\n",
    "        threshold  = 0.05)\n",
    "        \n",
    "'''\n",
    "\n",
    "d_06orig2 ='''```{.python .input}\n",
    "library(rcompanion)\n",
    "\n",
    "cldList(comparison = PT$Comparison,\n",
    "        p.value    =\n",
    "PT$P.adj,\n",
    "        threshold  = 0.05)\n",
    "```'''\n",
    "d_06new2 ='''#CODE BLOCK COMMENTED OFF BECAUSE CANNOT AUTOMATE RUN DUE TO `Error: No significant differences.` AS OUTPUT.\n",
    "library(rcompanion)\n",
    "\n",
    "cldList(comparison = PT$Comparison,\n",
    "        p.value    =\n",
    "PT$P.adj,\n",
    "        threshold  = 0.05)\n",
    "\n",
    "'''\n",
    "\n",
    "d_07orig1 ='''```{.python .input}\n",
    "### additional model checking plots\n",
    "with: plot(model)\n",
    "```'''\n",
    "d_07new1 ='''### additional model checking plots\n",
    "\n",
    "`plot(model)`'''\n",
    "\n",
    "d_07orig2 ='''cld(leastsquare,\n",
    "    alpha=0.05,\n",
    "    Letters=letters,      ### Use lower-case letters for .group\n",
    "    adjust=\"tukey\")       ### Tukey-adjusted comparisons'''\n",
    "d_07new2 ='''library(emmeans)\n",
    "library(multcomp)\n",
    "\n",
    "em = emmeans(model, ~ Tech)\n",
    "\n",
    "cld(em,\n",
    "    alpha=0.05,\n",
    "    Letters=letters,      ### Use lower-case letters for .group\n",
    "    adjust=\"tukey\")       ### Tukey-adjusted comparisons'''\n",
    "d_07orig3 ='''library(TukeyC)\n",
    "\n",
    "tuk = TukeyC(Data,\n",
    "             model = 'Protein ~ Tech + Error(Rat)',\n",
    "             error = 'Rat',\n",
    "             which = 'Tech',\n",
    "             fl1=1,\n",
    "             sig.level = 0.05)\n",
    "\n",
    "summary(tuk)'''\n",
    "d_07new3 ='''library(TukeyC)\n",
    "\n",
    "tuk = TukeyC(Protein ~ Tech + Error(Rat),\n",
    "             data = Data,\n",
    "             which = 'Tech',\n",
    "             error = 'Rat',)\n",
    "\n",
    "summary(tuk)'''\n",
    "\n",
    "d_08orig1 ='''lsmeans = lmerTest::lsmeans ### Uses the lsmeans function'''\n",
    "d_08new1 ='''lsmeans = lmerTest::lsmeansLT ### Uses the lsmeans function'''\n",
    "d_08orig2 ='''### Extract lsmeans table\n",
    "Sum = PT$diffs.lsmeans.table\n",
    "\n",
    "\n",
    "### Extract comparisons and p-values\n",
    "  Comparison = rownames(Sum)\n",
    "\n",
    "P.value   = Sum$'p-value'\n",
    "\n",
    "\n",
    "### Adjust p-values\n",
    "P.value.adj = p.adjust(P.value,\n",
    "                       method =  \"none\"[)   \n",
    "\n",
    "]{style=\"color:blue\"}### Fix names of comparisons\n",
    "Comparison = gsub(\"-\"[, ]{style=\"color:blue\"}\"- Day\"[, Comparison)\n",
    "\n",
    "\n",
    "]{style=\"color:blue\"}### Produce compact letter display\n",
    "library(rcompanion)\n",
    "\n",
    "cldList(comparison = Comparison,\n",
    "        p.value    = P.value.adj,\n",
    "        threshold = 0.05)'''\n",
    "d_08new2 ='''### Extract lsmeans table\n",
    "Sum = PT$diffs.lsmeans.table\n",
    "\n",
    "\n",
    "### Extract comparisons and p-values\n",
    "  Comparison = rownames(Sum)\n",
    "\n",
    "P.value   = Sum$'p-value'\n",
    "\n",
    "\n",
    "### Adjust p-values\n",
    "P.value.adj = p.adjust(P.value,\n",
    "                       method =  \"none\")   \n",
    "\n",
    "### Fix names of comparisons\n",
    "Comparison = gsub(\"-\", \"- Day\", Comparison)\n",
    "\n",
    "\n",
    "### Produce compact letter display\n",
    "library(rcompanion)\n",
    "\n",
    "cldList(comparison = Comparison,\n",
    "        p.value    = P.value.adj,\n",
    "        threshold = 0.05)'''\n",
    "d_08orig3 ='''library(multcompView)\n",
    "library(lsmeans)\n",
    "\n",
    "lsmeans = lsmeans::lsmeans ### Uses the lsmeans  function                           \n",
    "###  from the lsmeans package,\n",
    "                            ###  not from the lmerTest package\n",
    "leastsquare = lsmeans(model, \n",
    "                      pairwise ~ Sex:Genotype,\n",
    "                      adjust=\"tukey\")\n",
    "\n",
    " \n",
    "\n",
    "cld(leastsquare, \n",
    "    alpha=.05,  \n",
    "    Letters=letters)'''\n",
    "d_08new3 ='''library(emmeans)\n",
    "library(multcomp)\n",
    "\n",
    "ieff = emmeans(model, ~ Sex:Genotype)\n",
    "\n",
    "cld(ieff, alpha=.05, Letters=letters)'''\n",
    "d_08orig4 ='''library(multcompView)\n",
    "library(lsmeans)\n",
    "\n",
    "lsmeans = lsmeans::lsmeans ### Uses the lsmeans function\n",
    "                           ###  from the lsmeans package,\n",
    "                           ###  not from the lmerTest package\n",
    "leastsquare = lsmeans(model, \n",
    "                     pairwise ~ Day,\n",
    "                     alpha=.05,\n",
    "                     adjust=\"tukey\")\n",
    "\n",
    "cld(leastsquare, \n",
    "    alpha=.05,  \n",
    "    Letters=letters,\n",
    "    adjust=\"tukey\")'''\n",
    "d_08new4 ='''library(emmeans)\n",
    "library(multcomp)\n",
    "\n",
    "em = emmeans(model, ~ Day)\n",
    "\n",
    "cld(em, alpha=.05, Letters=letters)'''\n",
    "\n",
    "d_08aorig1 ='''```{.python .input}\n",
    "### Order groups by median\n",
    " \n",
    "Data$Factor.A = factor(Data$Factor.A, \n",
    "                     \n",
    "levels = c(\"n\", \"l\", \"m\"))\n",
    "\n",
    "\n",
    "### Pairwise robust test\n",
    "  library(rcompanion)\n",
    "\n",
    "PT = pairwiseRobustTest(\n",
    "     Response ~ Factor.A, \n",
    "     data = Data,\n",
    "     est=\"mom\", \n",
    "     nboot=5000,\n",
    "     method=\"fdr\")      # adjust p-values; see ?p.adjust for options\n",
    "PT\n",
    "```'''\n",
    "d_08anew1 ='''\n",
    "### Order groups by median\n",
    " \n",
    "Data$Factor.A = factor(Data$Factor.A, \n",
    "                     \n",
    "levels = c(\"n\", \"l\", \"m\"))\n",
    "\n",
    "\n",
    "### Pairwise robust test\n",
    "  library(rcompanion)\n",
    "\n",
    "PT = pairwiseRobustTest(\n",
    "     Response ~ Factor.A, \n",
    "     data = Data,\n",
    "     est=\"mom\", \n",
    "     nboot=5000,\n",
    "     method=\"fdr\")      # adjust p-values; see ?p.adjust for options\n",
    "PT\n",
    "'''\n",
    "d_08aorig2 ='''```{.python .input}\n",
    "### Produce compact letter display \n",
    "library(rcompanion)\n",
    "\n",
    "cldList(comparison = PT$Comparison,\n",
    "        p.value    = PT$p.adjust,\n",
    "        threshold  = 0.05)\n",
    "```'''\n",
    "d_08anew2 ='''\n",
    "### Produce compact letter display \n",
    "library(rcompanion)\n",
    "\n",
    "cldList(comparison = PT$Comparison,\n",
    "        p.value    = PT$p.adjust,\n",
    "        threshold  = 0.05)\n",
    "\n",
    "'''\n",
    "d_08aorig3 ='''```{.python .input}\n",
    "library(rcompanion)\n",
    "\n",
    "PT = pairwiseRobustTest(\n",
    "     Response ~ Factor.int, \n",
    "     data = Data,\n",
    "     est=\"mom\", \n",
    "     nboot=5000,\n",
    "     method=\"fdr\")      # adjust p-values; see ?p.adjust for options\n",
    "PT\n",
    "```'''\n",
    "d_08anew3 ='''\n",
    "library(rcompanion)\n",
    "\n",
    "PT = pairwiseRobustTest(\n",
    "     Response ~ Factor.int, \n",
    "     data = Data,\n",
    "     est=\"mom\", \n",
    "     nboot=5000,\n",
    "     method=\"fdr\")      # adjust p-values; see ?p.adjust for options\n",
    "PT\n",
    "\n",
    "'''\n",
    "d_08aorig5 ='''```{.python .input}\n",
    "### Order groups by median\n",
    " \n",
    "Data$Factor = factor(Data$Factor, \n",
    "                     \n",
    "levels = c(\"n\", \"l\", \"m\"))\n",
    "\n",
    "\n",
    "### Pairwise robust tests\n",
    "  library(rcompanion)\n",
    "\n",
    "PM = pairwiseRobustMatrix(\n",
    "     Response ~ Factor.A, \n",
    "     data = Data,\n",
    "     est=\"mom\", \n",
    "     nboot=5000,\n",
    "     method=\"fdr\")      # adjust p-values; see ?p.adjust for options\n",
    "PM$Adjusted\n",
    "```'''\n",
    "d_08anew5 ='''\n",
    "### Order groups by median\n",
    " \n",
    "Data$Factor = factor(Data$Factor, \n",
    "                     \n",
    "levels = c(\"n\", \"l\", \"m\"))\n",
    "\n",
    "\n",
    "### Pairwise robust tests\n",
    "  library(rcompanion)\n",
    "\n",
    "PM = pairwiseRobustMatrix(\n",
    "     Response ~ Factor.A, \n",
    "     data = Data,\n",
    "     est=\"mom\", \n",
    "     nboot=5000,\n",
    "     method=\"fdr\")      # adjust p-values; see ?p.adjust for options\n",
    "PM$Adjusted\n",
    "'''\n",
    "d_08aorig6 ='''```{.python .input}\n",
    "library(multcompView)\n",
    "\n",
    "multcompLetters(PM$Adjusted,\n",
    "                compare=\"<\",\n",
    "                threshold=0.05,\n",
    "                Letters=letters,\n",
    "                reversed = FALSE)\n",
    "```'''\n",
    "d_08anew6 ='''\n",
    "library(multcompView)\n",
    "\n",
    "multcompLetters(PM$Adjusted,\n",
    "                compare=\"<\",\n",
    "                threshold=0.05,\n",
    "                Letters=letters,\n",
    "                reversed = FALSE)\n",
    "\n",
    "'''\n",
    "d_08aorig7 ='''```{.python .input}\n",
    "### Order groups by median\n",
    "  Data$Factor.int = factor(Data$Factor.int,\n",
    "                         \n",
    "levels = c(\"m.y\", \"m.x\", \"n.x\", \"l.y\", \"n.y\", \"l.x\"))\n",
    "\n",
    "\n",
    "### Pairwise robust tests\n",
    "  library(rcompanion)\n",
    "\n",
    "PM = pairwiseRobustMatrix(\n",
    "     Response ~ Factor.int, \n",
    "     data = Data,\n",
    "     est=\"mom\", \n",
    "     nboot=5000,\n",
    "     method=\"fdr\")      # adjust p-values; see ?p.adjust for options\n",
    "PM\n",
    "```'''\n",
    "d_08anew7 ='''\n",
    "### Order groups by median\n",
    "  Data$Factor.int = factor(Data$Factor.int,\n",
    "                         \n",
    "levels = c(\"m.y\", \"m.x\", \"n.x\", \"l.y\", \"n.y\", \"l.x\"))\n",
    "\n",
    "\n",
    "### Pairwise robust tests\n",
    "  library(rcompanion)\n",
    "\n",
    "PM = pairwiseRobustMatrix(\n",
    "     Response ~ Factor.int, \n",
    "     data = Data,\n",
    "     est=\"mom\", \n",
    "     nboot=5000,\n",
    "     method=\"fdr\")      # adjust p-values; see ?p.adjust for options\n",
    "PM\n",
    "'''\n",
    "\n",
    "e_01orig1 ='''pwr.r.test(n = NULL, '''\n",
    "e_01new1 ='''library(pwr)\n",
    "pwr.r.test(n = NULL, '''\n",
    "\n",
    "e_03orig1 ='''### additional model checking plots\n",
    "with: plot(model.final)'''\n",
    "e_03new1 ='''### additional model checking plots with:\n",
    "plot(model.final)'''\n",
    "e_03orig2 ='''### additional model checking plots\n",
    "with: plot(model)'''\n",
    "e_03new2 ='''### additional model checking plots with:\n",
    "plot(model)'''\n",
    "\n",
    "e_04orig1 ='''### additional model checking plots with: plot(model.2)\n",
    "### alternative: library(FSA);\n",
    "residPlot(model.2)'''\n",
    "e_04new1 ='''### additional model checking plots with: \n",
    "plot(model.2)\n",
    "### alternative: \n",
    "library(FSA);\n",
    "residPlot(model.2)'''\n",
    "\n",
    "e_05orig2 ='''compare.lm'''\n",
    "e_05new2 ='''compareLM'''\n",
    "\n",
    "e_06orig1 ='''### additional model checking plots\n",
    "with: plot(model)'''\n",
    "e_06new1 ='''### additional model checking plots with:\n",
    "plot(model)'''\n",
    "\n",
    "e_07orig1 ='''Species   Status Length\n",
    "Mass Range Migr Insect Diet Clutch Broods Wood Upland Water Release Indiv\n",
    " Cyg_olor  1      1520  9600 1.21  1    12     2     6     1      0    0     \n",
    "1      6        29\n",
    " Cyg_atra  1      1250  5000 0.56  1     0     1     6     1      0    0     \n",
    "1     10        85\n",
    " Cer_nova  1       870  3360 0.07  1     0     1     4     1      0    0     \n",
    "1      3         8\n",
    " Ans_caer  0       720  2517 1.1   3    12     2     3.8   1      0    0     \n",
    "1      1        10\n",
    " Ans_anse  0       820  3170 3.45  3     0     1     5.9   1      0    0     \n",
    "1      2         7\n",
    " Bra_cana  1       770  4390 2.96  2     0     1     5.9   1      0    0     \n",
    "1     10        60\n",
    " Bra_sand  0        50  1930 0.01  1     0     1     4     2      0    0     \n",
    "0      1         2\n",
    " Alo_aegy  0       680  2040 2.71  1    NA     2     8.5   1      0    0     \n",
    "1      1         8\n",
    " Ana_plat  1       570  1020 9.01  2     6     2    12.6   1      0    0     \n",
    "1     17      1539\n",
    " Ana_acut  0       580   910 7.9   3     6     2     8.3   1      0    0     \n",
    "1      3       102\n",
    " Ana_pene  0       480   590 4.33  3     0     1     8.7   1      0    0     \n",
    "1      5        32\n",
    " Aix_spon  0       470   539 1.04  3    12     2    13.5   2      1    0     \n",
    "1      5        10\n",
    " Ayt_feri  0       450   940 2.17  3    12     2     9.5   1      0    0     \n",
    "1      3         9\n",
    " Ayt_fuli  0       435   684 4.81  3    12     2    10.1   1      0    0     \n",
    "1      2         5\n",
    " Ore_pict  0       275   230 0.31  1     3     1     9.5   1      1    1     \n",
    "0      9       398\n",
    " Lop_cali  1       256   162 0.24  1     3     1    14.2   2      0    0     \n",
    "0     15      1420\n",
    " Col_virg  1       230   170 0.77  1     3     1    13.7   1      0    0     \n",
    "0     17      1156\n",
    " Ale_grae  1       330   501 2.23  1     3     1    15.5   1      0    1     \n",
    "0     15       362\n",
    " Ale_rufa  0       330   439 0.22  1     3     2    11.2   2      0    0     \n",
    "0      2        20\n",
    " Per_perd  0       300   386 2.4   1     3     1    14.6   1      0    1     \n",
    "0     24       676\n",
    " Cot_pect  0       182    95 0.33  3    NA     2     7.5   1      0    0     \n",
    "0      3      NA\n",
    " Cot_aust  1       180    95 0.69  2    12     2    11     1      0    0     \n",
    "1     11       601\n",
    " Lop_nyct  0       800  1150 0.28  1    12     2     5     1      1    1     \n",
    "0      4         6\n",
    " Pha_colc  1       710   850 1.25  1    12     2    11.8   1      1    0     \n",
    "0     27       244\n",
    " Syr_reev  0       750   949 0.2   1    12     2     9.5   1      1    1     \n",
    "0      2         9\n",
    " Tet_tetr  0       470   900 4.17  1     3     1     7.9   1      1    1     \n",
    "0      2        13\n",
    " Lag_lago  0       390   517 7.29  1     0     1     7.5   1      1    1     \n",
    "0      2         4\n",
    " Ped_phas  0       440   815 1.83  1     3     1    12.3   1      1    0     \n",
    "0      1        22\n",
    " Tym_cupi  0       435   770 0.26  1     4     1    12     1      0    0     \n",
    "0      3        57\n",
    " Van_vane  0       300   226 3.93  2    12     3     3.8   1      0    0     \n",
    "0      8       124\n",
    " Plu_squa  0       285   318 1.67  3    12     3     4     1      0    0     \n",
    "1      2         3\n",
    " Pte_alch  0       350   225 1.21  2     0     1     2.5   2      0    0     \n",
    "0      1         8\n",
    " Pha_chal  0       320   350 0.6   1    12     2     2     2      1    0     \n",
    "0      8        42\n",
    " Ocy_loph  0       330   205 0.76  1     0     1     2     7      1    0     \n",
    "1      4        23\n",
    " Leu_mela  0       372  NA   0.07  1    12     2     2     1      1    0      0     \n",
    "6        34\n",
    " Ath_noct  1       220   176 4.84  1    12     3     3.6   1      1    0     \n",
    "0      7       221\n",
    " Tyt_alba  0       340   298 8.9   2     0     3     5.7   2      1    0     \n",
    "0      1         7\n",
    " Dac_nova  1       460   382 0.34  1    12     3     2     1      1    0     \n",
    "0      7        21\n",
    " Lul_arbo  0       150  32.1 1.78  2     4     2     3.9   2      1    0     \n",
    "0      1         5\n",
    " Ala_arve  1       185  38.9 5.19  2    12     2     3.7   3      0    0     \n",
    "0     11       391\n",
    " Pru_modu  1       145  20.5 1.95  2    12     2     3.4   2      1    0     \n",
    "0     14       245\n",
    " Eri_rebe  0       140  15.8 2.31  2    12     2     5     2      1    0     \n",
    "0     11       123\n",
    " Lus_mega  0       161  19.4 1.88  3    12     2     4.7   2      1    0     \n",
    "0      4         7\n",
    " Tur_meru  1       255  82.6 3.3   2    12     2     3.8   3      1    0     \n",
    "0     16       596\n",
    " Tur_phil  1       230  67.3 4.84  2    12     2     4.7   2      1    0     \n",
    "0     12       343\n",
    " Syl_comm  0       140  12.8 3.39  3    12     2     4.6   2      1    0     \n",
    "0      1         2\n",
    " Syl_atri  0       142  17.5 2.43  2     5     2     4.6   1      1    0     \n",
    "0      1         5\n",
    " Man_mela  0       180  NA   0.04  1    12     3     1.9   5      1    0     \n",
    "0      1         2\n",
    " Man_mela  0       265    59 0.25  1    12     2     2.6   NA     1    0     \n",
    "0      1        80\n",
    " Gra_cyan  0       275   128 0.83  1    12     3     3     2      1    0     \n",
    "1      1      NA\n",
    " Gym_tibi  1       400   380 0.82  1    12     3     4     1      1    0     \n",
    "0     15       448\n",
    " Cor_mone  0       335   203 3.4   2    12     2     4.5   1      1    0     \n",
    "0      2         3\n",
    " Cor_frug  1       400   425 3.73  1    12     2     3.6   1      1    0     \n",
    "0     10       182 \n",
    " Stu_vulg  1       222  79.8 3.33  2     6     2     4.8   2      1    0     \n",
    "0     14       653\n",
    " Acr_tris  1       230 111.3 0.56  1    12     2     3.7   1      1    0     \n",
    "0      5        88\n",
    " Pas_dome  1       149  28.8 6.5   1     6     2     3.9   3      1    0     \n",
    "0     12       416\n",
    " Pas_mont  0       133    22 6.8   1     6     2     4.7   3      1    0     \n",
    "0      3        14\n",
    " Aeg_temp  0       120  NA   0.17  1     6     2     4.7   3      1    0     \n",
    "0      3        14\n",
    " Emb_gutt  0       120    19 0.15  1     4     1     5     3      0    0     \n",
    "0      4       112\n",
    " Poe_gutt  0       100  12.4 0.75  1     4     1     4.7   3      0    0     \n",
    "0      1        12\n",
    " Lon_punc  0       110  13.5 1.06  1     0     1     5     3      0    0     \n",
    "0      1         8\n",
    " Lon_cast  0       100  NA   0.13  1     4     1     5     NA     0    0     \n",
    "1      4        45\n",
    " Pad_oryz  0       160  NA   0.09  1     0     1     5     NA     0    0     \n",
    "0      2         6\n",
    " Fri_coel  1       160  23.5 2.61  2    12     2     4.9   2      1    0     \n",
    "0     17       449\n",
    " Fri_mont  0       146  21.4 3.09  3    10     2     6     NA     1    0     \n",
    "0      7       121\n",
    " Car_chlo  1       147  29   2.09  2     7     2     4.8   2      1    0     \n",
    "0      6        65\n",
    " Car_spin  0       117  12   2.09  3     3     1     4     2      1    0     \n",
    "0      3        54\n",
    " Car_card  1       120  15.5 2.85  2     4     1     4.4   3      1    0     \n",
    "0     14       626\n",
    " Aca_flam  1       115  11.5 5.54  2     6     1     5     2      1    0     \n",
    "0     10       607\n",
    " Aca_flavi 0       133  17   1.67  2     0     1     5     3      0    1     \n",
    "0      3        61\n",
    " Aca_cann  0       136  18.5 2.52  2     6     1     4.7   2      1    0     \n",
    "0     12       209\n",
    " Pyr_pyrr  0       142  23.5 3.57  1     4     1     4     3      1    0      0     \n",
    "2      NA\n",
    " Emb_citr  1       160  28.2 4.11  2     8     2     3.3   3      1    0     \n",
    "0     14       656\n",
    " Emb_hort  0       163  21.6 2.75  3    12     2     5     1      0    0     \n",
    "0      1         6\n",
    " Emb_cirl  1       160  23.6 0.62  1    12     2     3.5   2      1    0     \n",
    "0      3        29\n",
    " Emb_scho  0       150  20.7 5.42  1    12     2     5.1   2      0    0     \n",
    "1      2         9\n",
    " Pir_rubr  0       170  31   0.55  3    12     2     4     NA     1    0     \n",
    "0      1         2\n",
    " Age_phoe  0       210  36.9 2     2     8     2     3.7   1      0    0     \n",
    "1      1         2\n",
    " Stu_negl  0       225 106.5 1.2   2    12     2     4.8   2      0    0     \n",
    "0      1         2'''\n",
    "e_07new1 ='''Species   Status Length Mass Range Migr Insect Diet Clutch Broods Wood Upland Water Release Indiv\n",
    " Cyg_olor  1      1520  9600 1.21  1    12     2     6     1      0    0      1      6        29\n",
    " Cyg_atra  1      1250  5000 0.56  1     0     1     6     1      0    0      1     10        85\n",
    " Cer_nova  1       870  3360 0.07  1     0     1     4     1      0    0      1      3         8\n",
    " Ans_caer  0       720  2517 1.1   3    12     2     3.8   1      0    0      1      1        10\n",
    " Ans_anse  0       820  3170 3.45  3     0     1     5.9   1      0    0      1      2         7\n",
    " Bra_cana  1       770  4390 2.96  2     0     1     5.9   1      0    0      1     10        60\n",
    " Bra_sand  0        50  1930 0.01  1     0     1     4     2      0    0      0      1         2\n",
    " Alo_aegy  0       680  2040 2.71  1    NA     2     8.5   1      0    0      1      1         8\n",
    " Ana_plat  1       570  1020 9.01  2     6     2    12.6   1      0    0      1     17      1539\n",
    " Ana_acut  0       580   910 7.9   3     6     2     8.3   1      0    0      1      3       102\n",
    " Ana_pene  0       480   590 4.33  3     0     1     8.7   1      0    0      1      5        32\n",
    " Aix_spon  0       470   539 1.04  3    12     2    13.5   2      1    0      1      5        10\n",
    " Ayt_feri  0       450   940 2.17  3    12     2     9.5   1      0    0      1      3         9\n",
    " Ayt_fuli  0       435   684 4.81  3    12     2    10.1   1      0    0      1      2         5\n",
    " Ore_pict  0       275   230 0.31  1     3     1     9.5   1      1    1      0      9       398\n",
    " Lop_cali  1       256   162 0.24  1     3     1    14.2   2      0    0      0     15      1420\n",
    " Col_virg  1       230   170 0.77  1     3     1    13.7   1      0    0      0     17      1156\n",
    " Ale_grae  1       330   501 2.23  1     3     1    15.5   1      0    1      0     15       362\n",
    " Ale_rufa  0       330   439 0.22  1     3     2    11.2   2      0    0      0      2        20\n",
    " Per_perd  0       300   386 2.4   1     3     1    14.6   1      0    1      0     24       676\n",
    " Cot_pect  0       182    95 0.33  3    NA     2     7.5   1      0    0      0      3      NA\n",
    " Cot_aust  1       180    95 0.69  2    12     2    11     1      0    0      1     11       601\n",
    " Lop_nyct  0       800  1150 0.28  1    12     2     5     1      1    1      0      4         6\n",
    " Pha_colc  1       710   850 1.25  1    12     2    11.8   1      1    0      0     27       244\n",
    " Syr_reev  0       750   949 0.2   1    12     2     9.5   1      1    1      0      2         9\n",
    " Tet_tetr  0       470   900 4.17  1     3     1     7.9   1      1    1      0      2        13\n",
    " Lag_lago  0       390   517 7.29  1     0     1     7.5   1      1    1      0      2         4\n",
    " Ped_phas  0       440   815 1.83  1     3     1    12.3   1      1    0      0      1        22\n",
    " Tym_cupi  0       435   770 0.26  1     4     1    12     1      0    0      0      3        57\n",
    " Van_vane  0       300   226 3.93  2    12     3     3.8   1      0    0      0      8       124\n",
    " Plu_squa  0       285   318 1.67  3    12     3     4     1      0    0      1      2         3\n",
    " Pte_alch  0       350   225 1.21  2     0     1     2.5   2      0    0      0      1         8\n",
    " Pha_chal  0       320   350 0.6   1    12     2     2     2      1    0      0      8        42\n",
    " Ocy_loph  0       330   205 0.76  1     0     1     2     7      1    0      1      4        23\n",
    " Leu_mela  0       372  NA   0.07  1    12     2     2     1      1    0      0      6        34\n",
    " Ath_noct  1       220   176 4.84  1    12     3     3.6   1      1    0      0      7       221\n",
    " Tyt_alba  0       340   298 8.9   2     0     3     5.7   2      1    0      0      1         7\n",
    " Dac_nova  1       460   382 0.34  1    12     3     2     1      1    0      0      7        21\n",
    " Lul_arbo  0       150  32.1 1.78  2     4     2     3.9   2      1    0      0      1         5\n",
    " Ala_arve  1       185  38.9 5.19  2    12     2     3.7   3      0    0      0     11       391\n",
    " Pru_modu  1       145  20.5 1.95  2    12     2     3.4   2      1    0      0     14       245\n",
    " Eri_rebe  0       140  15.8 2.31  2    12     2     5     2      1    0      0     11       123\n",
    " Lus_mega  0       161  19.4 1.88  3    12     2     4.7   2      1    0      0      4         7\n",
    " Tur_meru  1       255  82.6 3.3   2    12     2     3.8   3      1    0      0     16       596\n",
    " Tur_phil  1       230  67.3 4.84  2    12     2     4.7   2      1    0      0     12       343\n",
    " Syl_comm  0       140  12.8 3.39  3    12     2     4.6   2      1    0      0      1         2\n",
    " Syl_atri  0       142  17.5 2.43  2     5     2     4.6   1      1    0      0      1         5\n",
    " Man_mela  0       180  NA   0.04  1    12     3     1.9   5      1    0      0      1         2\n",
    " Man_mela  0       265    59 0.25  1    12     2     2.6   NA     1    0      0      1        80\n",
    " Gra_cyan  0       275   128 0.83  1    12     3     3     2      1    0      1      1      NA\n",
    " Gym_tibi  1       400   380 0.82  1    12     3     4     1      1    0      0     15       448\n",
    " Cor_mone  0       335   203 3.4   2    12     2     4.5   1      1    0      0      2         3\n",
    " Cor_frug  1       400   425 3.73  1    12     2     3.6   1      1    0      0     10       182 \n",
    " Stu_vulg  1       222  79.8 3.33  2     6     2     4.8   2      1    0      0     14       653\n",
    " Acr_tris  1       230 111.3 0.56  1    12     2     3.7   1      1    0      0      5        88\n",
    " Pas_dome  1       149  28.8 6.5   1     6     2     3.9   3      1    0      0     12       416\n",
    " Pas_mont  0       133    22 6.8   1     6     2     4.7   3      1    0      0      3        14\n",
    " Aeg_temp  0       120  NA   0.17  1     6     2     4.7   3      1    0      0      3        14\n",
    " Emb_gutt  0       120    19 0.15  1     4     1     5     3      0    0      0      4       112\n",
    " Poe_gutt  0       100  12.4 0.75  1     4     1     4.7   3      0    0      0      1        12\n",
    " Lon_punc  0       110  13.5 1.06  1     0     1     5     3      0    0      0      1         8\n",
    " Lon_cast  0       100  NA   0.13  1     4     1     5     NA     0    0      1      4        45\n",
    " Pad_oryz  0       160  NA   0.09  1     0     1     5     NA     0    0      0      2         6\n",
    " Fri_coel  1       160  23.5 2.61  2    12     2     4.9   2      1    0      0     17       449\n",
    " Fri_mont  0       146  21.4 3.09  3    10     2     6     NA     1    0      0      7       121\n",
    " Car_chlo  1       147  29   2.09  2     7     2     4.8   2      1    0      0      6        65\n",
    " Car_spin  0       117  12   2.09  3     3     1     4     2      1    0      0      3        54\n",
    " Car_card  1       120  15.5 2.85  2     4     1     4.4   3      1    0      0     14       626\n",
    " Aca_flam  1       115  11.5 5.54  2     6     1     5     2      1    0      0     10       607\n",
    " Aca_flavi 0       133  17   1.67  2     0     1     5     3      0    1      0      3        61\n",
    " Aca_cann  0       136  18.5 2.52  2     6     1     4.7   2      1    0      0     12       209\n",
    " Pyr_pyrr  0       142  23.5 3.57  1     4     1     4     3      1    0      0      2      NA\n",
    " Emb_citr  1       160  28.2 4.11  2     8     2     3.3   3      1    0      0     14       656\n",
    " Emb_hort  0       163  21.6 2.75  3    12     2     5     1      0    0      0      1         6\n",
    " Emb_cirl  1       160  23.6 0.62  1    12     2     3.5   2      1    0      0      3        29\n",
    " Emb_scho  0       150  20.7 5.42  1    12     2     5.1   2      0    0      1      2         9\n",
    " Pir_rubr  0       170  31   0.55  3    12     2     4     NA     1    0      0      1         2\n",
    " Age_phoe  0       210  36.9 2     2     8     2     3.7   1      0    0      1      1         2\n",
    " Stu_negl  0       225 106.5 1.2   2    12     2     4.8   2      0    0      0      1         2'''\n",
    "e_07orig2 = '''Species   Status Length\n",
    "Mass Range Migr Insect Diet Clutch Broods Wood Upland Water Release Indiv\n",
    " Cyg_olor  1      1520  9600 1.21  1    12     2     6     1      0    0     \n",
    "1      6        29\n",
    " Cyg_atra  1      1250  5000 0.56  1     0     1     6     1      0    0     \n",
    "1     10        85\n",
    " Cer_nova  1       870  3360 0.07  1     0     1     4     1      0    0     \n",
    "1      3         8\n",
    " Ans_caer  0       720  2517 1.1   3    12     2     3.8   1      0    0     \n",
    "1      1        10\n",
    " Ans_anse  0       820  3170 3.45  3     0     1     5.9   1      0    0     \n",
    "1      2         7\n",
    " Bra_cana  1       770  4390 2.96  2     0     1     5.9   1      0    0     \n",
    "1     10        60\n",
    " Bra_sand  0        50  1930 0.01  1     0     1     4     2      0    0     \n",
    "0      1         2\n",
    " Alo_aegy  0       680  2040 2.71  1    NA     2     8.5   1      0    0     \n",
    "1      1         8\n",
    " Ana_plat  1       570  1020 9.01  2     6     2    12.6   1      0    0     \n",
    "1     17      1539\n",
    " Ana_acut  0       580   910 7.9   3     6     2     8.3   1      0    0     \n",
    "1      3       102\n",
    " Ana_pene  0       480   590 4.33  3     0     1     8.7   1      0    0     \n",
    "1      5        32\n",
    " Aix_spon  0       470   539 1.04  3    12     2    13.5   2      1    0     \n",
    "1      5        10\n",
    " Ayt_feri  0       450   940 2.17  3    12     2     9.5   1      0    0     \n",
    "1      3         9\n",
    " Ayt_fuli  0       435   684 4.81  3    12     2    10.1   1      0    0     \n",
    "1      2         5\n",
    " Ore_pict  0       275   230 0.31  1     3     1     9.5   1      1    1     \n",
    "0      9       398\n",
    " Lop_cali  1       256   162 0.24  1     3     1    14.2   2      0    0     \n",
    "0     15      1420\n",
    " Col_virg  1       230   170 0.77  1     3     1    13.7   1      0    0      0    \n",
    "17      1156\n",
    " Ale_grae  1       330   501 2.23  1     3     1    15.5   1      0    1     \n",
    "0     15       362\n",
    " Ale_rufa  0       330   439 0.22  1     3     2    11.2   2      0    0     \n",
    "0      2        20\n",
    " Per_perd  0       300   386 2.4   1     3     1    14.6   1      0    1     \n",
    "0     24       676\n",
    " Cot_pect  0       182    95 0.33  3    NA     2     7.5   1      0    0     \n",
    "0      3      NA\n",
    " Cot_aust  1       180    95 0.69  2    12     2    11     1      0    0     \n",
    "1     11       601\n",
    " Lop_nyct  0       800  1150 0.28  1    12     2     5     1      1    1     \n",
    "0      4         6\n",
    " Pha_colc  1       710   850 1.25  1    12     2    11.8   1      1    0     \n",
    "0     27       244\n",
    " Syr_reev  0       750   949 0.2   1    12     2     9.5   1      1    1     \n",
    "0      2         9\n",
    " Tet_tetr  0       470   900 4.17  1     3     1     7.9   1      1    1     \n",
    "0      2        13\n",
    " Lag_lago  0       390   517 7.29  1     0     1     7.5   1      1    1     \n",
    "0      2         4\n",
    " Ped_phas  0       440   815 1.83  1     3     1    12.3   1      1    0     \n",
    "0      1        22\n",
    " Tym_cupi  0       435   770 0.26  1     4     1    12     1      0    0     \n",
    "0      3        57\n",
    " Van_vane  0       300   226 3.93  2    12     3     3.8   1      0    0     \n",
    "0      8       124\n",
    " Plu_squa  0       285   318 1.67  3    12     3     4     1      0    0     \n",
    "1      2         3\n",
    " Pte_alch  0       350   225 1.21  2     0     1     2.5   2      0    0     \n",
    "0      1         8\n",
    " Pha_chal  0       320   350 0.6   1    12     2     2     2      1    0     \n",
    "0      8        42\n",
    " Ocy_loph  0       330   205 0.76  1     0     1     2     7      1    0     \n",
    "1      4        23\n",
    " Leu_mela  0       372  NA   0.07  1    12     2     2     1      1    0     \n",
    "0      6        34\n",
    " Ath_noct  1       220   176 4.84  1    12     3     3.6   1      1    0     \n",
    "0      7       221\n",
    " Tyt_alba  0       340   298 8.9   2     0     3     5.7   2      1    0     \n",
    "0      1         7\n",
    " Dac_nova  1       460   382 0.34  1    12     3     2     1      1    0     \n",
    "0      7        21\n",
    " Lul_arbo  0       150  32.1 1.78  2     4     2     3.9   2      1    0     \n",
    "0      1         5\n",
    " Ala_arve  1       185  38.9 5.19  2    12     2     3.7   3      0    0     \n",
    "0     11       391\n",
    " Pru_modu  1       145  20.5 1.95  2    12     2     3.4   2      1    0     \n",
    "0     14       245\n",
    " Eri_rebe  0       140  15.8 2.31  2    12     2     5     2      1    0     \n",
    "0     11       123\n",
    " Lus_mega  0       161  19.4 1.88  3    12     2     4.7   2      1    0     \n",
    "0      4         7\n",
    " Tur_meru  1       255  82.6 3.3   2    12     2     3.8   3      1    0     \n",
    "0     16       596\n",
    " Tur_phil  1       230  67.3 4.84  2    12     2     4.7   2      1    0     \n",
    "0     12       343\n",
    " Syl_comm  0       140  12.8 3.39  3    12     2     4.6   2      1    0     \n",
    "0      1         2\n",
    " Syl_atri  0       142  17.5 2.43  2     5     2     4.6   1      1    0     \n",
    "0      1         5\n",
    " Man_mela  0       180  NA   0.04  1    12     3     1.9   5      1    0     \n",
    "0      1         2\n",
    " Man_mela  0       265    59 0.25  1    12     2     2.6   NA     1    0     \n",
    "0      1        80\n",
    " Gra_cyan  0       275   128 0.83  1    12     3     3     2      1    0     \n",
    "1      1      NA\n",
    " Gym_tibi  1       400   380 0.82  1    12     3     4     1      1    0     \n",
    "0     15       448\n",
    " Cor_mone  0       335   203 3.4   2    12     2     4.5   1      1    0     \n",
    "0      2         3\n",
    " Cor_frug  1       400   425 3.73  1    12     2     3.6   1      1    0     \n",
    "0     10       182 \n",
    " Stu_vulg  1       222  79.8 3.33  2     6     2     4.8   2      1    0      0    \n",
    "14       653\n",
    " Acr_tris  1       230 111.3 0.56  1    12     2     3.7   1      1    0     \n",
    "0      5        88\n",
    " Pas_dome  1       149  28.8 6.5   1     6     2     3.9   3      1    0     \n",
    "0     12       416\n",
    " Pas_mont  0       133    22 6.8   1     6     2     4.7   3      1    0     \n",
    "0      3        14\n",
    " Aeg_temp  0       120  NA   0.17  1     6     2     4.7   3      1    0     \n",
    "0      3        14\n",
    " Emb_gutt  0       120    19 0.15  1     4     1     5     3      0    0     \n",
    "0      4       112\n",
    " Poe_gutt  0       100  12.4 0.75  1     4     1     4.7   3      0    0     \n",
    "0      1        12\n",
    " Lon_punc  0       110  13.5 1.06  1     0     1     5     3      0    0     \n",
    "0      1         8\n",
    " Lon_cast  0       100  NA   0.13  1     4     1     5     NA     0    0     \n",
    "1      4        45\n",
    " Pad_oryz  0       160  NA   0.09  1     0     1     5     NA     0    0     \n",
    "0      2         6\n",
    " Fri_coel  1       160  23.5 2.61  2    12     2     4.9   2      1    0     \n",
    "0     17       449\n",
    " Fri_mont  0       146  21.4 3.09  3    10     2     6     NA     1    0     \n",
    "0      7       121\n",
    " Car_chlo  1       147  29   2.09  2     7     2     4.8   2      1    0     \n",
    "0      6        65\n",
    " Car_spin  0       117  12   2.09  3     3     1     4     2      1    0     \n",
    "0      3        54\n",
    " Car_card  1       120  15.5 2.85  2     4     1     4.4   3      1    0     \n",
    "0     14       626\n",
    " Aca_flam  1       115  11.5 5.54  2     6     1     5     2      1    0     \n",
    "0     10       607\n",
    " Aca_flavi 0       133  17   1.67  2     0     1     5     3      0    1     \n",
    "0      3        61\n",
    " Aca_cann  0       136  18.5 2.52  2     6     1     4.7   2      1    0     \n",
    "0     12       209\n",
    " Pyr_pyrr  0       142  23.5 3.57  1     4     1     4     3      1    0     \n",
    "0      2      NA\n",
    " Emb_citr  1       160  28.2 4.11  2     8     2     3.3   3      1    0     \n",
    "0     14       656\n",
    " Emb_hort  0       163  21.6 2.75  3    12     2     5     1      0    0     \n",
    "0      1         6\n",
    " Emb_cirl  1       160  23.6 0.62  1    12     2     3.5   2      1    0     \n",
    "0      3        29\n",
    " Emb_scho  0       150  20.7 5.42  1    12     2     5.1   2      0    0     \n",
    "1      2         9\n",
    " Pir_rubr  0       170  31   0.55  3    12     2     4     NA     1    0     \n",
    "0      1         2\n",
    " Age_phoe  0       210  36.9 2     2     8     2     3.7   1      0    0     \n",
    "1      1         2\n",
    " Stu_negl  0       225 106.5 1.2   2    12     2     4.8   2      0    0     \n",
    "0      1         2'''\n",
    "\n",
    "e_07orig3 ='''summary(model)'''\n",
    "e_07new3 ='''summary(model.final)'''\n",
    "\n",
    "h_01orig1='''```{.python .input}\n",
    "Input = \"\n",
    "Contrast          Merlot  Cabernet  Syrah  Chardonnay  Riesling \n",
    "GewuÌrtztraminer'''\n",
    "\n",
    "h_01new1='''```{.python .input}\n",
    "Input = \"\n",
    "Contrast          Merlot  Cabernet  Syrah  Chardonnay  Riesling  GewuÌrtztraminer'''\n",
    "h_01orig2='''\"\n",
    "\n",
    " names match the order of levels of the treatment variable\n",
    "   ### The coefficients of each row sum to 0\n",
    "\n",
    "Matriz'''\n",
    "\n",
    "h_01new2='''\"\n",
    "\n",
    "   ### The column names match the order of levels of the treatment variable\n",
    "   ### The coefficients of each row sum to 0\n",
    "\n",
    "Matriz'''\n",
    "\n",
    "\n",
    "repaird_dict = {\"a_06.md\":[[a_06orig1,a_06new1],[a_06orig2,a_06new2],[a_06orig3,a_06new3]],\n",
    "                \"a_07.md\":[[a_07orig1,a_07new1],],\n",
    "                \"a_08.md\":[[a_08orig1,a_08new1],],\n",
    "                \"b_09.md\":[[']{style=\"color:#006600\"}','']],\n",
    "                \"d_05.md\":[[d_05orig1,d_05new1],[d_05orig2,d_05new2],[d_05orig3,d_05new3],[d_05orig4,d_05new4]],\n",
    "                \"d_06.md\":[[d_06orig1,d_06new1],[d_06orig2,d_06new2]],\n",
    "                \"d_07.md\":[[d_07orig1,d_07new1],[d_07orig2,d_07new2],[d_07orig3,d_07new3],],\n",
    "                \"d_08.md\":[[d_08orig1,d_08new1],[d_08orig2,d_08new2],[d_08orig3,d_08new3],[d_08orig4,d_08new4]],\n",
    "                \"d_08a.md\":[[d_08aorig1,d_08anew1],[d_08aorig2,d_08anew2],[d_08aorig3,d_08anew3],[d_08aorig5,d_08anew5],[d_08aorig6,d_08anew6],[d_08aorig7,d_08anew7],],\n",
    "                \"e_01.md\":[[e_01orig1,e_01new1],],\n",
    "                \"e_03.md\":[[e_03orig1,e_03new1],[e_03orig2,e_03new2]],\n",
    "                \"e_04.md\":[[e_04orig1,e_04new1],],\n",
    "                \"e_05.md\":[[e_03orig1,e_03new1],[e_05orig2,e_05new2],],\n",
    "                \"e_06.md\":[[e_06orig1,e_06new1],],\n",
    "                \"e_07.md\":[[e_07orig1,e_07new1],[e_07orig2,e_07new1],[e_07orig3,e_07new3]],\n",
    "                \"h_01.md\":[[h_01orig1,h_01new1],[h_01orig2,h_01new2]]}\n",
    "\n",
    "for md in repaird_dict:\n",
    "    with open(md, 'r') as input:\n",
    "        all_md=input.read()\n",
    "    for f in repaird_dict[md]:\n",
    "        if f[0] in all_md:\n",
    "            all_md = all_md.replace(f[0],f[1])\n",
    "        else:\n",
    "            sys.stderr.write(f\"Issue involving text `{f[0]}` seems no longer an \"\n",
    "                f\"issue in `{md}`\\nRemove the special handling for this.\")\n",
    "\n",
    "    %store all_md > {md}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP #5: Start it cycling through markdown files making the output notebooks (unrun)\n",
    "\n",
    "Because notedown makes the notebook, they won't be run. However, this allows the markdown generated earlier actually show as markdown in markdown cells that can be further edited, which I couldn't get from my earlier effort with Papermill alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCompanion_notebooks = []\n",
    "for md in RCompanion_markdowns:\n",
    "    notebook_name = f'{md.rsplit(\".md\",1)[0]}.ipynb'\n",
    "    !notedown --match=fenced {md} > {notebook_name}\n",
    "    RCompanion_notebooks .append(notebook_name)\n",
    "    # LATER USE nbconvert to run edited versions of the notebooks in repo where to be located\n",
    "    #!jupyter nbconvert --to notebook --execute {pair[0]} --output={pair[0]}\n",
    "    #nbconvert use based on\n",
    "    # https://nbconvert.readthedocs.io/en/latest/execute_api.html ; learned of\n",
    "    # `output=` from usage by running `!jupyter nbconvert` in a Jupyter cell.\n",
    "    # Otherwise, it was add `nbconvert` to file names in front of extension,\n",
    "    #  like so: `Drawing_ideograms_Part1.nbconvert.ipynb`.\n",
    "    '''\n",
    "    !papermill Papermill\\ Template\\ for\\ Circos\\ later\\ tutorial\\ notebooks.ipynb {pair[0]} -p markdown {markdown} -p section {section} -p sub_pg {sub_pg} -p end_nb {end_nb} -p next_nb {next_nb}\n",
    "    '''\n",
    "    sys.stderr.write(\"\\n*********************************************************\\n\"\n",
    "        \"Unrun notebook `{}` generated.\"\n",
    "        \"\\n********************************************************\"\n",
    "        \"*\\n\".format(notebook_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow up that, the notebooks each will later be run en masse [using nbconvert](https://nbconvert.readthedocs.io/en/latest/execute_api.html#executing-notebooks-from-the-command-line) to highlight if there are issues with individual ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all the results\n",
    "-----------------------------------\n",
    "\n",
    "Run the next cell to gather and archive both the produced initial draft `.ipynb` files and the edited markdown files.  \n",
    "Also collect the three lists used here as json files so the contents can be used for automating filling in the markdown into Jupyter ipynb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_file_name = \"unrun_notebooks_from_RCompanion.tar.gz\"\n",
    "import os\n",
    "import sys\n",
    "# store `RCompanion_urls` and `RCompanion_markdowns` and `RCompanion_notebooks`\n",
    "# as json since lighter-weight and more portable than pickling\n",
    "# and may be useful in later steps if automate some post-generation editing.\n",
    "RCompanion_urls_to_get_storedfn = \"RCompanion_urls_to_get.json\"\n",
    "RCompanion_markdowns_made_storedfn = \"RCompanion_markdowns_made.json\"\n",
    "RCompanion_notebooks_made_storedfn = \"RCompanion_notebooks_made.json\"\n",
    "\n",
    "import json\n",
    "with open(RCompanion_urls_to_get_storedfn, 'w') as f:\n",
    "    json.dump(RCompanion_urls, f)\n",
    "with open(RCompanion_markdowns_made_storedfn, 'w') as f:\n",
    "    json.dump(RCompanion_markdowns, f)\n",
    "with open(RCompanion_notebooks_made_storedfn, 'w') as f:\n",
    "    json.dump(RCompanion_notebooks, f)\n",
    "files_to_archive = RCompanion_notebooks + RCompanion_markdowns + [RCompanion_urls_to_get_storedfn] + [RCompanion_markdowns_made_storedfn] + [RCompanion_notebooks_made_storedfn]\n",
    "!tar czf {archive_file_name} {\" \".join(files_to_archive)}\n",
    "sys.stderr.write(\"***************************DONE***********************************\\n\"\n",
    "    \"'{}' generated. Download it.\\n\"\n",
    "    \"***************************DONE***********************************\".format(archive_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get organized**, now that everything has been used and processed (mostly).  \n",
    "Move the individual files to sub-directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir rcomp_html\n",
    "!mkdir rcomp_mds\n",
    "!mkdir rcomp_nbs\n",
    "\n",
    "def extract_name_of_the_html(url, add_html_extension):\n",
    "    '''\n",
    "    make a file name based on the URL \"https://rcompanion.org/rcompanion/index.html\".\n",
    "    if `add_html_extension` is True than add `.html` extension\n",
    "    to the file name.\n",
    "    \n",
    "    Return filename\n",
    "    '''\n",
    "    split_url = url.split(\"/\")\n",
    "    fn = split_url[-1]\n",
    "    if add_html_extension:\n",
    "        fn += \".html\"\n",
    "    return fn\n",
    "\n",
    "\n",
    "for url in RCompanion_urls:\n",
    "    fn = extract_name_of_the_html(url, add_html_extension=False)\n",
    "    !mv {fn} rcomp_html/.\n",
    "for fn in RCompanion_markdowns:\n",
    "    !mv {fn} rcomp_mds/.\n",
    "for fn in RCompanion_notebooks:\n",
    "    !mv {fn} rcomp_nbs/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "----\n",
    "\n",
    "## Post-processing\n",
    "\n",
    "Go on to three.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
