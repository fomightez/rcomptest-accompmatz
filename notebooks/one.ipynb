{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting R Comp html and making install.R file\n",
    "\n",
    "This can be run [here](https://github.com/fomightez/muscle-binder) where I added the recent version of pandoc on Feb 3, 2019. That version added conversion to markdown.\n",
    "\n",
    "This is to make a binder-launchable environment with all the necessary R packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/c8/a55eb6ea11cd7e5ac4bacdf92bac4693b90d3ba79268be16527555e186f0/beautifulsoup4-4.8.1-py3-none-any.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 3.2MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting soupsieve>=1.2 (from beautifulsoup4)\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/42/d821581cf568e9b7dfc5b415aa61952b0f5e3dede4f3cbd650e3a1082992/soupsieve-1.9.4-py2.py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.8.1 soupsieve-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_companion_index_url = \"https://rcompanion.org/rcompanion/index.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'the_html' (str) to file 'index.html'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'https://rcompanion.org/rcompanion/index.html': 'Purpose of this book',\n",
       " 'https://rcompanion.org/rcompanion/a_02.html': 'The Handbook for Biological Statistics',\n",
       " 'https://rcompanion.org/rcompanion/a_03.html': 'About the author',\n",
       " 'https://rcompanion.org/rcompanion/a_04.html': 'About R',\n",
       " 'https://rcompanion.org/rcompanion/a_05.html': 'Obtaining R',\n",
       " 'https://rcompanion.org/rcompanion/a_06.html': 'A Few Notes to Get Started with R',\n",
       " 'https://rcompanion.org/rcompanion/a_07.html': 'Avoiding Pitfalls in R',\n",
       " 'https://rcompanion.org/rcompanion/a_08.html': 'Help with R',\n",
       " 'https://rcompanion.org/rcompanion/a_09.html': 'R Tutorials',\n",
       " 'https://rcompanion.org/rcompanion/a_10.html': 'Formal Statistics Books',\n",
       " 'https://rcompanion.org/rcompanion/b_01.html': 'Exact Test of Goodness-of-Fit',\n",
       " 'https://rcompanion.org/rcompanion/b_02.html': 'Power Analysis',\n",
       " 'https://rcompanion.org/rcompanion/b_03.html': 'Chi-square Test of Goodness-of-Fit',\n",
       " 'https://rcompanion.org/rcompanion/b_04.html': 'G–test of goodness-of-fit',\n",
       " 'https://rcompanion.org/rcompanion/b_05.html': 'Chi-square Test of Independence',\n",
       " 'https://rcompanion.org/rcompanion/b_06.html': 'G–test of Independence',\n",
       " 'https://rcompanion.org/rcompanion/b_07.html': 'Fisher’s Exact Test of Independence',\n",
       " 'https://rcompanion.org/rcompanion/b_08.html': 'Small Numbers in Chi-square and G–tests',\n",
       " 'https://rcompanion.org/rcompanion/b_09.html': 'Repeated G–tests of Goodness-of-Fit',\n",
       " 'https://rcompanion.org/rcompanion/b_10.html': 'Cochran–Mantel–Haenszel Test for Repeated Tests of Independence',\n",
       " 'https://rcompanion.org/rcompanion/c_01.html': 'Statistics of Central Tendency',\n",
       " 'https://rcompanion.org/rcompanion/c_02.html': 'Statistics of Dispersion',\n",
       " 'https://rcompanion.org/rcompanion/c_03.html': 'Standard Error of the Mean',\n",
       " 'https://rcompanion.org/rcompanion/c_04.html': 'Confidence Limits',\n",
       " 'https://rcompanion.org/rcompanion/d_01.html': 'Student’s t–test for One Sample',\n",
       " 'https://rcompanion.org/rcompanion/d_02.html': 'Student’s t–test for Two Samples',\n",
       " 'https://rcompanion.org/rcompanion/d_02a.html': 'Mann–Whitney and Two-sample Permutation Test',\n",
       " 'https://rcompanion.org/rcompanion/d_03.html': 'Chapters Not Covered in This Book',\n",
       " 'https://rcompanion.org/rcompanion/d_04.html': 'Type I, II, and III Sums of Squares',\n",
       " 'https://rcompanion.org/rcompanion/d_05.html': 'One-way Anova',\n",
       " 'https://rcompanion.org/rcompanion/d_06.html': 'Kruskal–Wallis Test',\n",
       " 'https://rcompanion.org/rcompanion/d_06a.html': 'One-way Analysis with Permutation Test',\n",
       " 'https://rcompanion.org/rcompanion/d_07.html': 'Nested Anova',\n",
       " 'https://rcompanion.org/rcompanion/d_08.html': 'Two-way Anova',\n",
       " 'https://rcompanion.org/rcompanion/d_08a.html': 'Two-way Anova with Robust Estimation',\n",
       " 'https://rcompanion.org/rcompanion/d_09.html': 'Paired t–test',\n",
       " 'https://rcompanion.org/rcompanion/d_10.html': 'Wilcoxon Signed-rank Test',\n",
       " 'https://rcompanion.org/rcompanion/e_01.html': 'Correlation and Linear Regression',\n",
       " 'https://rcompanion.org/rcompanion/e_02.html': 'Spearman Rank Correlation',\n",
       " 'https://rcompanion.org/rcompanion/e_03.html': 'Curvilinear Regression',\n",
       " 'https://rcompanion.org/rcompanion/e_04.html': 'Analysis of Covariance',\n",
       " 'https://rcompanion.org/rcompanion/e_05.html': 'Multiple Regression',\n",
       " 'https://rcompanion.org/rcompanion/e_06.html': 'Simple Logistic Regression',\n",
       " 'https://rcompanion.org/rcompanion/e_07.html': 'Multiple Logistic Regression',\n",
       " 'https://rcompanion.org/rcompanion/f_01.html': 'Multiple Comparisons',\n",
       " 'https://rcompanion.org/rcompanion/g_01.html': 'Chapters Not Covered in This Book',\n",
       " 'https://rcompanion.org/rcompanion/h_01.html': 'Contrasts in Linear Models',\n",
       " 'https://rcompanion.org/rcompanion/h_02.html': 'Cate–Nelson Analysis',\n",
       " 'https://rcompanion.org/rcompanion/i_01.html': 'Reading SAS Datalines in R',\n",
       " 'http://rcompanion.org/handbook/': 'Summary and Analysis of Extension Program Evaluation in R'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_prefix = \"https://rcompanion.org/rcompanion/\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "def extract_name_of_the_html(url, add_html_extension):\n",
    "    '''\n",
    "    make a file name based on the URL \"https://rcompanion.org/rcompanion/index.html\".\n",
    "    if `add_html_extension` is True than add `.html` extension\n",
    "    to the file name.\n",
    "    \n",
    "    Return filename\n",
    "    '''\n",
    "    split_url = url.split(\"/\")\n",
    "    fn = split_url[-1]\n",
    "    if add_html_extension:\n",
    "        fn += \".html\"\n",
    "    return fn\n",
    "\n",
    "def get_html_and_save(url):\n",
    "    '''\n",
    "    Take a url for a web page get the html and stores the text.\n",
    "    Returns the html code too\n",
    "    \n",
    "    based on https://stackoverflow.com/a/30890016/8508004\n",
    "    '''\n",
    "    global the_html # so can save using `%store` the variable needs to be global\n",
    "    global fn_save_name # so can save using `%store` the variable needs to be global\n",
    "    hh = urllib.request.urlopen(url)\n",
    "    hbytes = hh.read()\n",
    "\n",
    "    the_html = hbytes.decode(\"utf8\")\n",
    "    #print (the_html[:200])\n",
    "    hh.close()\n",
    "    \n",
    "    fn_save_name = extract_name_of_the_html(url, add_html_extension=False)\n",
    "    \n",
    "    %store the_html > {fn_save_name}\n",
    "    \n",
    "    return the_html \n",
    "\n",
    "\n",
    "pages_and_titles_dict = {}\n",
    "index_html = get_html_and_save(r_companion_index_url)\n",
    "# mine from the Contents panel on the left, the list of the pages\n",
    "nav_code = index_html.split(\"<!-- Begin Navigation -->\")[1].split(\"<!-- End Navigation -->\")[0]\n",
    "contents_code = nav_code.split(\"<ul>Introduction\")[1].split('<div id=\"adskyscraper\">')[0]\n",
    "#print(nav_code )\n",
    "\n",
    "# ul and li tags based on https://stackoverflow.com/a/17246983/8508004\n",
    "soup = BS(nav_code)\n",
    "for ultag in soup.find_all('ul'):\n",
    "    for litag in ultag.find_all('li'):\n",
    "        #print(litag.text.strip())  #<--ends up being same as `print(link.text.strip())`\n",
    "        pass\n",
    "        for link in litag.find_all('a'):\n",
    "            #print(link.get('title')) #based on https://stackoverflow.com/a/32542575/8508004\n",
    "            #print(link.text.strip())\n",
    "            #print(link.get('href')) #based on https://python.gotrained.com/beautifulsoup-extracting-urls/\n",
    "            if link.get('href').startswith(\"http://rcompanion.org/\"):\n",
    "                full_link = link.get('href')\n",
    "            else:\n",
    "                full_link = f\"{site_prefix}{link.get('href')}\"\n",
    "            pages_and_titles_dict[full_link] = link.text.strip()\n",
    "pages_and_titles_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, do not remove a few that I already converted to notebooks because I need to get packages those need, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drafts_made_already = [\\n    \"Reading_SAS_Datalines_in_R.ipynb\",\\n    \"Power_Analysis.ipynb\",\\n    \"Exact_Test_of_Goodness-of-Fit.ipynb\"\\n    \\n]\\n\\ndrafts_made_already_for_matching = [x.rsplit(\".ipynb\")[0].replace(\"_\",\" \") for x in drafts_made_already]\\nones_to_remove = [] # need to make a list because cannot delete while iterating over\\nfor url,page_name in pages_and_titles_dict.items():\\n    if page_name in drafts_made_already_for_matching:\\n        ones_to_remove.append(url)\\nfor t in ones_to_remove:\\n    del pages_and_titles_dict[t]\\npages_and_titles_dict\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''drafts_made_already = [\n",
    "    \"Reading_SAS_Datalines_in_R.ipynb\",\n",
    "    \"Power_Analysis.ipynb\",\n",
    "    \"Exact_Test_of_Goodness-of-Fit.ipynb\"\n",
    "    \n",
    "]\n",
    "\n",
    "drafts_made_already_for_matching = [x.rsplit(\".ipynb\")[0].replace(\"_\",\" \") for x in drafts_made_already]\n",
    "ones_to_remove = [] # need to make a list because cannot delete while iterating over\n",
    "for url,page_name in pages_and_titles_dict.items():\n",
    "    if page_name in drafts_made_already_for_matching:\n",
    "        ones_to_remove.append(url)\n",
    "for t in ones_to_remove:\n",
    "    del pages_and_titles_dict[t]\n",
    "pages_and_titles_dict\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the last one that leads to another set of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_specifically = \"http://rcompanion.org/handbook/\"\n",
    "del pages_and_titles_dict[remove_specifically]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now get the html for each and make markdown from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_get = list(pages_and_titles_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def extract_name_of_the_html(url, add_html_extension):\n",
    "    '''\n",
    "    make a file name based on the URL \"https://rcompanion.org/rcompanion/index.html\".\n",
    "    if `add_html_extension` is True than add `.html` extension\n",
    "    to the file name.\n",
    "    \n",
    "    Return filename\n",
    "    '''\n",
    "    split_url = url.split(\"/\")\n",
    "    fn = split_url[-1]\n",
    "    if add_html_extension:\n",
    "        fn += \".html\"\n",
    "    return fn\n",
    "\n",
    "def get_html_and_save(url):\n",
    "    '''\n",
    "    Take a url for a web page get the html and store the text.\n",
    "    \n",
    "    return the name of the html and the name of file to save.\n",
    "    (Turns out `%store` magics didn't work in the function?!)\n",
    "    \n",
    "    based on https://stackoverflow.com/a/30890016/8508004\n",
    "    '''\n",
    "    hh = urllib.request.urlopen(url)\n",
    "    hbytes = hh.read()\n",
    "\n",
    "    the_html = hbytes.decode(\"utf8\")\n",
    "    #print (the_html[:200])\n",
    "    hh.close()\n",
    "    fn_save_name = extract_name_of_the_html(url, add_html_extension=False)\n",
    "    \n",
    "    #%store the_html > {fn_save_name} #seems cannot use this in a function?;\n",
    "    # probably because it needs to be a global and here it would be local\n",
    "    # variable it would be trying to save.\n",
    "    \n",
    "    return the_html,fn_save_name\n",
    "\n",
    "htmls_collected = []\n",
    "markdowns_made = []\n",
    "for url in urls_to_get:\n",
    "    the_html,fn_save_name = get_html_and_save(url)\n",
    "    %store the_html > {fn_save_name}\n",
    "    htmls_collected.append(fn_save_name)\n",
    "    markdown_name = fn_save_name.rsplit(\".html\")[0] + \".md\"\n",
    "    !pandoc -s -f html -t markdown {fn_save_name} -o {markdown_name}\n",
    "    sys.stderr.write(\"'{}' has been generated.\\n\".format(markdown_name))\n",
    "    markdowns_made.append(markdown_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over markdown produced and collect packages needed for running all R code\n",
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#markdowns_made = [\"d_06.md\"] # Uncomment for debugging only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RVAideMemoire',\n",
       " 'lsmeans',\n",
       " 'Rmisc',\n",
       " 'DescTools',\n",
       " 'agricolae',\n",
       " 'FSA',\n",
       " 'car',\n",
       " 'pwr',\n",
       " 'WRS2',\n",
       " 'TukeyC',\n",
       " 'XNomial',\n",
       " 'lmerTest',\n",
       " 'dplyr',\n",
       " 'psych',\n",
       " 'multcompView',\n",
       " 'rcompanion',\n",
       " 'coin',\n",
       " 'nlme',\n",
       " 'ggplot2',\n",
       " 'vcd',\n",
       " 'lme4',\n",
       " 'grid',\n",
       " 'BSDA',\n",
       " 'multcomp']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_libraries_from_md(md):\n",
    "    '''\n",
    "    Go through markdown line by line and collect packages needed from lines like:\n",
    "    \n",
    "    if(!require(DescTools)){install.packages(\\\"DescTools\\\")}\\\n",
    "    if(!require(RVAideMemoire)){install.packages(\\\"RVAideMemoire\\\")}\\\n",
    "    \n",
    "    Return packages as list\n",
    "    '''\n",
    "    packages_needed = []\n",
    "    with open(md, 'r') as input:\n",
    "        for line in input:\n",
    "            if (line.strip().startswith(\"if(!require\")) and (\n",
    "                \"{install.packages(\" in line):\n",
    "                package_nom = line.split('{install.packages(')[1].split('\\\")}')[0].strip()\n",
    "                packages_needed.append(package_nom[2:-1])\n",
    "    return packages_needed\n",
    "\n",
    "R_packages = []\n",
    "for md in markdowns_made:\n",
    "    extracted_packages = extract_libraries_from_md(md)\n",
    "    R_packages += extracted_packages\n",
    "R_packages = list(set(R_packages))\n",
    "R_packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the install.R file for the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'text_2_save' (str) to file 'install.R'.\n"
     ]
    }
   ],
   "source": [
    "basic_line = 'install.packages(\"PLACEHOLDER\")'\n",
    "text_2_save = ''\n",
    "for x in R_packages:\n",
    "    text_2_save += basic_line.replace(\"PLACEHOLDER\",x)\n",
    "    text_2_save += \"\\n\"\n",
    "%store text_2_save > install.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `install.R` for placing in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "archive_file_name = \"FirstSetmarkdown_from_RCompanion.tar.gz\"\n",
    "import os\n",
    "import sys\n",
    "# store `urls_to_get` and `markdowns_made` as json since lighter-weight and more portable than pickling\n",
    "# and the order of them wll correspond to the index I made so I can use them with papermill \n",
    "# in conjuction without needing to make a new dictionary.\n",
    "RCompanion_urls_to_get_storedfn = \"RCompanion_urls_to_get.json\"\n",
    "RCompanion_markdowns_made_storedfn = \"RCompanion_markdowns_made.json\"\n",
    "import json\n",
    "with open(RCompanion_urls_to_get_storedfn, 'w') as f:\n",
    "    json.dump(urls_to_get, f)\n",
    "with open(RCompanion_markdowns_made_storedfn, 'w') as f:\n",
    "    json.dump(markdowns_made, f)\n",
    "files_to_archive = markdowns_made + [RCompanion_urls_to_get_storedfn] + [RCompanion_markdowns_made_storedfn]\n",
    "!tar czf {archive_file_name} {\" \".join(files_to_archive)}\n",
    "sys.stderr.write(\"***************************DONE***********************************\\n\"\n",
    "    \"'{}' generated. Download it.\\n\"\n",
    "    \"***************************DONE***********************************\".format(archive_file_name))\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow-up this with `?????`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ece0737a9832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mexecuteSomething\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-ece0737a9832>\u001b[0m in \u001b[0;36mexecuteSomething\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#60 seconds times 8 minutes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def executeSomething():\n",
    "    #code here\n",
    "    print ('.')\n",
    "    time.sleep(480) #60 seconds times 8 minutes\n",
    "\n",
    "while True:\n",
    "    executeSomething()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
